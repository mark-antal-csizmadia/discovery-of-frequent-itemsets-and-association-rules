{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import *\n",
    "from pyspark.sql.functions import desc, col, rand\n",
    "from pyspark.sql import *\n",
    "from graphframes import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sympy.ntheory.generate import nextprime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from math import comb\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
    "# https://stackoverflow.com/questions/65011599/how-to-start-graphframes-on-spark-on-pyspark-on-juypter-on-docker\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 19:04:50 WARN Utils: Your hostname, mark-machine resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)\n",
      "21/11/18 19:04:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mark/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-74be1d16-eec7-4fc0-a83a-297022d361ed;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.1-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 105ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.1-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-74be1d16-eec7-4fc0-a83a-297022d361ed\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "21/11/18 19:04:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('hw2') \\\n",
    "        .master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/T10I4D100K.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "nprocs = mp.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    # read raw data\n",
    "    data_raw_rdd = spark.sparkContext.textFile(data_path)\n",
    "    # transform raw data to list of lists (list of ordered baskets)\n",
    "    data_rdd = data_raw_rdd.map(lambda x: set([int(y) for y in x.strip().split(\" \")]))\n",
    "    # get item counts (k:v where k is hashed item id and v is count)\n",
    "    items_counts_rdd = data_rdd.flatMap(lambda list: list).map(lambda w: (w,1)).reduceByKey(lambda a, b: a+b)\n",
    "    # get items\n",
    "    items_rdd = items_counts_rdd.map(lambda x: x[0])\n",
    "    \n",
    "    return data_rdd, items_rdd, items_counts_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_rdd, items_rdd, items_counts_rdd = read_data(data_path=data_path)\n",
    "data_rdd_c = data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{448, 834, 164, 775, 328, 687, 240, 368, 274, 561, 52, 630, 730, 825, 538, 25}, {704, 834, 581, 39, 205, 814, 401, 120, 825, 124}]\n",
      "n_baskets = 100000\n"
     ]
    }
   ],
   "source": [
    "print(data_rdd.take(2))\n",
    "n_baskets = data_rdd.count()\n",
    "print(f\"n_baskets = {n_baskets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_items = 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "items_rdd.take(20)\n",
    "n_items = items_rdd.count()\n",
    "print(f\"n_items = {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(448, 1370),\n",
       " (834, 1373),\n",
       " (164, 744),\n",
       " (328, 663),\n",
       " (240, 1399),\n",
       " (368, 7828),\n",
       " (274, 2628),\n",
       " (52, 1983),\n",
       " (630, 1523),\n",
       " (538, 3982)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(items_counts_rdd.count())\n",
    "items_counts_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_singletons(items_counts_rdd, s):\n",
    "    singletons_rdd = items_counts_rdd.filter(lambda x: s <= x[1])\n",
    "    singletons_rdd = singletons_rdd.map(lambda x: (set([x[0]]), x[1]))\n",
    "    return singletons_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_itemsets(k, itemsets_frequent_rdd, from_ckpt=False):\n",
    "    \n",
    "    assert 1 < k\n",
    "    \n",
    "    if from_ckpt:\n",
    "        candidates = np.load(f'ckpt/candidates_k_{k}.npy', allow_pickle=True)\n",
    "        candidates = spark.sparkContext.parallelize(candidates.tolist())\n",
    "        print(f\"loaded proposed n={candidates.count()} candidates\")\n",
    "    else:\n",
    "        singletons_rdd = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == 1)\n",
    "        k_minus_1_tons_rdd = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == k-1)\n",
    "        l1 = singletons_rdd.map(lambda x: x[0])\n",
    "        l2 = k_minus_1_tons_rdd.map(lambda x: x[0])\n",
    "        l3 = l1.cartesian(l2)\n",
    "\n",
    "        l4 = l3.map(lambda x: x[0].union(x[1])).filter(lambda x: len(x) == k)\n",
    "        l4c = l4.collect()\n",
    "        l6c = [set(item) for item in set(frozenset(item) for item in l4c)]\n",
    "        len(l6c)\n",
    "        #l6czip = [(idx, x) for idx, x in enumerate(l6c)]\n",
    "        l6czip = [(x, 0) for idx, x in enumerate(l6c)]\n",
    "        candidates = spark.sparkContext.parallelize(l6czip)\n",
    "\n",
    "        n_before_prune = candidates.count()\n",
    "\n",
    "        np.save(f'ckpt/candidates_notpruned_k_{k}', np.array(candidates.collect()))\n",
    "\n",
    "\n",
    "        for i in range(1, k):\n",
    "            n_comb = comb(k,i)\n",
    "            itemsets_i = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == i).collect()\n",
    "            candidates = candidates.map(lambda x: (x[0], sum([len(x[0].intersection(s[0])) == i for s in itemsets_i])))\\\n",
    "                .filter(lambda x: n_comb == x[1])\n",
    "\n",
    "        candidates = candidates.map(lambda x: x[0]).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "        n_after_prune = candidates.count()\n",
    "\n",
    "        np.save(f'ckpt/candidates_k_{k}', np.array(candidates.collect()))\n",
    "\n",
    "        print(f\"proposing n={candidates.count()} candidates (n_pruned={n_before_prune - n_after_prune})\")\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def f(candidate, data_rdd_c, k):\n",
    "        return len(list(filter(lambda x: len(x) == k, map(lambda x: x & candidate, data_rdd_c))))\n",
    "\n",
    "def filter_itemsets(candidates_rdd, k, s, itemsets_frequent_rdd, from_ckpt=False):\n",
    "    if from_ckpt:\n",
    "        start_time = time.time()\n",
    "        print(\"Filtering loading from file...\")\n",
    "        res = np.load(f'ckpt/filtered_candidates_k_{k}_s_{s}.npy', allow_pickle=True)\n",
    "        res = spark.sparkContext.parallelize(res.tolist())\n",
    "        end_time = time.time()\n",
    "        print(f\"k={k}, t={end_time - start_time}, n={res.count()}\")\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        print(\"Staring filtering...\")\n",
    "\n",
    "        candidates = candidates_rdd.collect()\n",
    "\n",
    "        pool = mp.Pool(processes=nprocs)\n",
    "        supports = pool.starmap(f, [(c,data_rdd_c,k) for (idx, c) in candidates])\n",
    "\n",
    "        res = \\\n",
    "            spark.sparkContext.parallelize(candidates)\\\n",
    "            .filter(lambda x: s <= supports[x[0]]).map(lambda x: (x[1], supports[x[0]]))\n",
    "\n",
    "        np.save(f'ckpt/filtered_candidates_k_{k}_s_{s}', np.array(res.collect()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"k={k}, t={end_time - start_time}, n={res.count()}\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1000\n",
    "from_ckpt = True\n",
    "\n",
    "itemsets_frequent_rdd_1 = get_singletons(items_counts_rdd=items_counts_rdd, s=s)\n",
    "itemsets_frequent_rdd_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({448}, 1370),\n",
       " ({834}, 1373),\n",
       " ({240}, 1399),\n",
       " ({368}, 7828),\n",
       " ({274}, 2628),\n",
       " ({52}, 1983),\n",
       " ({630}, 1523),\n",
       " ({538}, 3982),\n",
       " ({704}, 1794),\n",
       " ({814}, 1672)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded proposed n=70125 candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, {413, 494}],\n",
       " [1, {874, 978}],\n",
       " [2, {701, 946}],\n",
       " [3, {335, 804}],\n",
       " [4, {576, 583}],\n",
       " [5, {242, 684}],\n",
       " [6, {597, 641}],\n",
       " [7, {581, 766}],\n",
       " [8, {335, 538}],\n",
       " [9, {39, 884}],\n",
       " [10, {516, 854}],\n",
       " [11, {115, 735}],\n",
       " [12, {126, 952}],\n",
       " [13, {854, 895}],\n",
       " [14, {682, 740}],\n",
       " [15, {774, 984}],\n",
       " [16, {468, 984}],\n",
       " [17, {738, 749}],\n",
       " [18, {675, 790}],\n",
       " [19, {529, 600}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_2 = construct_itemsets(k=2, itemsets_frequent_rdd=itemsets_frequent_rdd_1, from_ckpt=from_ckpt)\n",
    "candidates_2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering loading from file...\n",
      "k=2, t=0.005522727966308594, n=9\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "new_itemsets_frequent_rdd_2 = \\\n",
    "    filter_itemsets(candidates_rdd=candidates_2, k=2, s=s, itemsets_frequent_rdd=itemsets_frequent_rdd_1, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_2 = itemsets_frequent_rdd_1.union(new_itemsets_frequent_rdd_2)\n",
    "print(itemsets_frequent_rdd_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded proposed n=1 candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, {39, 704, 825}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_3 = construct_itemsets(k=3, itemsets_frequent_rdd=itemsets_frequent_rdd_2, from_ckpt=from_ckpt)\n",
    "candidates_3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering loading from file...\n",
      "k=3, t=0.0051195621490478516, n=1\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "new_itemsets_frequent_rdd_3 = \\\n",
    "    filter_itemsets(candidates_rdd=candidates_3, k=3, s=s, itemsets_frequent_rdd=itemsets_frequent_rdd_2, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_3 = itemsets_frequent_rdd_2.union(new_itemsets_frequent_rdd_3)\n",
    "print(itemsets_frequent_rdd_3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
