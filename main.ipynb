{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import time\n",
    "from itertools import chain, combinations\n",
    "\n",
    "from utils import read_data\n",
    "from apriori import get_singletons, construct_itemsets_apriori, filter_itemsets_apriori\n",
    "from association_rules import get_subsets, get_association_rules, pretty_print_association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
    "# https://stackoverflow.com/questions/65011599/how-to-start-graphframes-on-spark-on-pyspark-on-juypter-on-docker\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 22:28:59 WARN Utils: Your hostname, mark-machine resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)\n",
      "21/11/18 22:28:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mark/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3863408f-c428-4368-8a52-c7fdd92e09fa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.1-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 108ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.1-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3863408f-c428-4368-8a52-c7fdd92e09fa\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "21/11/18 22:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('hw2').master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/T10I4D100K.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_rdd, items_rdd, items_counts_rdd = read_data(data_path=data_path, spark=spark)\n",
    "data_rdd_c = data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example basket:\n",
      "{448, 834, 164, 775, 328, 687, 240, 368, 274, 561, 52, 630, 730, 825, 538, 25}\n",
      "n_baskets = 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_items = 870\n",
      "Example item counts:\n",
      "[(448, 1370), (834, 1373), (164, 744), (328, 663), (240, 1399), (368, 7828), (274, 2628), (52, 1983), (630, 1523), (538, 3982)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"An example basket:\")\n",
    "print(data_rdd.take(1)[0])\n",
    "\n",
    "n_baskets = data_rdd.count()\n",
    "print(f\"n_baskets = {n_baskets}\")\n",
    "\n",
    "n_items = items_rdd.count()\n",
    "print(f\"n_items = {n_items}\")\n",
    "\n",
    "print(\"Example item counts:\")\n",
    "print(items_counts_rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori algorithm for finding frequent itemsets with a certain support threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support threshold = 1000\n",
      "number of singletons, first frequent itemset lenght=375 with support=1000\n",
      "Example singletons with their support\n",
      "[({448}, 1370), ({834}, 1373)]\n"
     ]
    }
   ],
   "source": [
    "# support threshold is 1000, 1% of n_baskets\n",
    "s = int(n_baskets * 0.01)\n",
    "print(f\"Support threshold = {s}\")\n",
    "# if load intermediate stuff like candidate itemsets and frequent itemsets, set to True\n",
    "from_ckpt = False\n",
    "\n",
    "itemsets_frequent_rdd_1 = get_singletons(items_counts_rdd=items_counts_rdd, s=s)\n",
    "print(f\"number of singletons, first frequent itemset lenght={itemsets_frequent_rdd_1.count()} \"\n",
    "      f\"with support={s}\")\n",
    "print(\"Example singletons with their support\")\n",
    "print(itemsets_frequent_rdd_1.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposing n=70125 candidates (n_pruned=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed candidate itemsets: number=70125 of lenght=2\n",
      "example candidates:\n",
      "[(0, {413, 494}), (1, {978, 874})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "candidates_2 = construct_itemsets_apriori(k=k, itemsets_frequent_rdd=itemsets_frequent_rdd_1, spark=spark, from_ckpt=from_ckpt)\n",
    "print(f\"proposed candidate itemsets: number={candidates_2.count()} of lenght={k}\")\n",
    "print(\"example candidates:\")\n",
    "print(candidates_2.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2, t=513.06393886 seconds, n=9\n",
      "new frequent itemsets: number=384, after adding n=9 filtered candidates of lenght k=2\n",
      "Example new frequent items with their support\n",
      "[({368, 829}, 1194), ({722, 390}, 1042)]\n"
     ]
    }
   ],
   "source": [
    "# this cell can take around 10 minutes\n",
    "k = 2\n",
    "new_itemsets_frequent_rdd_2 = \\\n",
    "    filter_itemsets_apriori(candidates_rdd=candidates_2, k=k, s=s, data_rdd_c=data_rdd_c, spark=spark, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_2 = itemsets_frequent_rdd_1.union(new_itemsets_frequent_rdd_2)\n",
    "print(f\"new frequent itemsets: number={itemsets_frequent_rdd_2.count()}, \"\n",
    "      f\"after adding n={new_itemsets_frequent_rdd_2.count()} filtered candidates of lenght k={k}\")\n",
    "print(\"Example new frequent items with their support\")\n",
    "print(new_itemsets_frequent_rdd_2.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 22:37:56 WARN TaskSetManager: Stage 31 contains a task of very large size (1007 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposing n=1 candidates (n_pruned=3351)\n",
      "proposed candidate itemsets: number=1 of lenght=3\n",
      "example candidates:\n",
      "[(0, {704, 825, 39})]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "candidates_3 = construct_itemsets_apriori(k=k, itemsets_frequent_rdd=itemsets_frequent_rdd_2, spark=spark, from_ckpt=from_ckpt)\n",
    "print(f\"proposed candidate itemsets: number={candidates_3.count()} of lenght={k}\")\n",
    "print(\"example candidates:\")\n",
    "print(candidates_3.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring filtering...\n",
      "k=3, t=0.50488234 seconds, n=1\n",
      "new frequent itemsets: number=385, after adding n=1 filtered candidates of lenght k=3\n",
      "Example new frequent items with their support\n",
      "[({704, 825, 39}, 1035)]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "new_itemsets_frequent_rdd_3 = \\\n",
    "    filter_itemsets_apriori(candidates_rdd=candidates_3, k=k, s=s, data_rdd_c=data_rdd_c, spark=spark, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_3 = itemsets_frequent_rdd_2.union(new_itemsets_frequent_rdd_3)\n",
    "print(f\"new frequent itemsets: number={itemsets_frequent_rdd_3.count()}, \"\n",
    "      f\"after adding n={new_itemsets_frequent_rdd_3.count()} filtered candidates of lenght k={k}\")\n",
    "print(\"Example new frequent items with their support\")\n",
    "print(new_itemsets_frequent_rdd_3.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 22:38:00 WARN TaskSetManager: Stage 54 contains a task of very large size (1007 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposing n=0 candidates (n_pruned=372)\n",
      "proposed candidate itemsets: number=0 of lenght=4\n",
      "example candidates:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "candidates_4 = construct_itemsets_apriori(k=k, itemsets_frequent_rdd=itemsets_frequent_rdd_3, spark=spark, from_ckpt=False)\n",
    "print(f\"proposed candidate itemsets: number={candidates_4.count()} of lenght={k}\")\n",
    "print(\"example candidates:\")\n",
    "print(candidates_4.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining association rules from frequent itemsets with a certain support and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frequent itemsets = 385\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of frequent itemsets = {itemsets_frequent_rdd_3.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({964}, 1518),\n",
       " ({708}, 1090),\n",
       " ({214}, 1893),\n",
       " ({208}, 1483),\n",
       " ({266}, 1022),\n",
       " ({458}, 1124),\n",
       " ({888}, 3686),\n",
       " ({334}, 2146),\n",
       " ({638}, 2288),\n",
       " ({790}, 1094)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_3.sample(withReplacement=False, fraction=0.1).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent itemset: {368, 829}\n",
      "\n",
      "frequent itemset: {722, 390}\n",
      "\n",
      "frequent itemset: {789, 829}\n",
      "\n",
      "frequent itemset: {704, 825}\n",
      "\tassociation rule: {704} -> {825} with confidence=0.6143\n",
      "\n",
      "frequent itemset: {704, 39}\n",
      "\tassociation rule: {704} -> {39} with confidence=0.6171\n",
      "\n",
      "frequent itemset: {227, 390}\n",
      "\tassociation rule: {227} -> {390} with confidence=0.5770\n",
      "\n",
      "frequent itemset: {368, 682}\n",
      "\n",
      "frequent itemset: {217, 346}\n",
      "\n",
      "frequent itemset: {825, 39}\n",
      "\n",
      "frequent itemset: {704, 825, 39}\n",
      "\tassociation rule: {704} -> {825, 39} with confidence=0.5769\n",
      "\tassociation rule: {704, 825} -> {39} with confidence=0.9392\n",
      "\tassociation rule: {704, 39} -> {825} with confidence=0.9350\n",
      "\tassociation rule: {825, 39} -> {704} with confidence=0.8719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# condidence threshold is 50%\n",
    "c_thresh = 0.5\n",
    "itemsets_frequent = itemsets_frequent_rdd_3.collect()\n",
    "\n",
    "association_rules = get_association_rules(itemsets_frequent=itemsets_frequent, c_thresh=c_thresh)\n",
    "pretty_print_association_rules(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
