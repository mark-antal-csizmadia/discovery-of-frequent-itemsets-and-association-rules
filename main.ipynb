{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import *\n",
    "from pyspark.sql.functions import desc, col, rand\n",
    "from pyspark.sql import *\n",
    "from graphframes import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sympy.ntheory.generate import nextprime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from math import comb\n",
    "import multiprocessing as mp\n",
    "\n",
    "from utils import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
    "# https://stackoverflow.com/questions/65011599/how-to-start-graphframes-on-spark-on-pyspark-on-juypter-on-docker\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('hw2').master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/T10I4D100K.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "# for multiprocessing\n",
    "nprocs = mp.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd, items_rdd, items_counts_rdd = read_data(data_path=data_path, spark=spark)\n",
    "data_rdd_c = data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example basket:\n",
      "{448, 834, 164, 775, 328, 687, 240, 368, 274, 561, 52, 630, 730, 825, 538, 25}\n",
      "n_baskets = 100000\n",
      "n_items = 870\n",
      "Example item counts:\n",
      "[(448, 1370), (834, 1373), (164, 744), (328, 663), (240, 1399), (368, 7828), (274, 2628), (52, 1983), (630, 1523), (538, 3982)]\n"
     ]
    }
   ],
   "source": [
    "print(\"An example basket:\")\n",
    "print(data_rdd.take(1)[0])\n",
    "\n",
    "n_baskets = data_rdd.count()\n",
    "print(f\"n_baskets = {n_baskets}\")\n",
    "\n",
    "n_items = items_rdd.count()\n",
    "print(f\"n_items = {n_items}\")\n",
    "\n",
    "print(\"Example item counts:\")\n",
    "print(items_counts_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_singletons(items_counts_rdd, s):\n",
    "    \"\"\"Read data given a path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    items_counts_rdd : pyspark.rdd.PipelinedRDD\n",
    "        A list of tuples of items and their counts. All items are represented as integers.\n",
    "\n",
    "    s : int\n",
    "        Support, min number of occurence across baskets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    singletons_rdd : pyspark.rdd.PipelinedRDD\n",
    "        A list of items and their counts/support.\n",
    "        E.g.:\n",
    "        [({448}, 1370),\n",
    "         ({834}, 1373),\n",
    "         ({240}, 1399),\n",
    "         ({368}, 7828),\n",
    "         ({274}, 2628),\n",
    "         ({52}, 1983),\n",
    "         ({630}, 1523),\n",
    "         ({538}, 3982),\n",
    "         ({704}, 1794),\n",
    "         ({814}, 1672)]\n",
    "    \"\"\"\n",
    "    singletons_rdd = items_counts_rdd.filter(lambda x: s <= x[1])\n",
    "    singletons_rdd = singletons_rdd.map(lambda x: (set([x[0]]), x[1]))\n",
    "    return singletons_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_itemsets(k, itemsets_frequent_rdd, from_ckpt=False):\n",
    "    \"\"\"Construct itemsets, or generate candidates for filtering in the Apriori algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Length of proposed itemsets.\n",
    "    \n",
    "    itemsets_frequent_rdd : pyspark.rdd.PipelinedRDD\n",
    "        A list of tuples of itemsets and their counts, from previous step. All items are represented as integers.\n",
    "\n",
    "    from_ckpt : bool\n",
    "        If True, load candidate sets from checkpoint, else compute them.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    candidates : pyspark.rdd.PipelinedRDD\n",
    "        A list of list where the first element is the id of the candidate itemset\n",
    "        and the second element is the itemset.\n",
    "        E.g.:\n",
    "        [[0, {413, 494}],\n",
    "         [1, {874, 978}],\n",
    "         [2, {701, 946}],\n",
    "         [3, {335, 804}],\n",
    "         [4, {576, 583}],\n",
    "         [5, {242, 684}],\n",
    "         [6, {597, 641}],\n",
    "         [7, {581, 766}],\n",
    "         [8, {335, 538}],\n",
    "         [9, {39, 884}],\n",
    "         [10, {516, 854}],\n",
    "         [11, {115, 735}],\n",
    "         [12, {126, 952}],\n",
    "         [13, {854, 895}],\n",
    "         [14, {682, 740}],\n",
    "         [15, {774, 984}],\n",
    "         [16, {468, 984}],\n",
    "         [17, {738, 749}],\n",
    "         [18, {675, 790}],\n",
    "         [19, {529, 600}]]\n",
    "    \"\"\"\n",
    "    assert 1 < k, f\"k={k}, needs to be 1<k\"\n",
    "    \n",
    "    if from_ckpt:\n",
    "        candidates = np.load(f'ckpt/candidates_k_{k}.npy', allow_pickle=True)\n",
    "        candidates = spark.sparkContext.parallelize(candidates.tolist())\n",
    "        print(f\"loaded proposed n={candidates.count()} candidates\")\n",
    "    else:\n",
    "        # get singelton itemsets\n",
    "        singletons_rdd = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == 1)\n",
    "        # get itemsets of length k-1\n",
    "        k_minus_1_tons_rdd = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == k-1)\n",
    "        # use singletions an k-1tons to prodcue cartesian product of itemsets\n",
    "        singletons_rdd = singletons_rdd.map(lambda x: x[0])\n",
    "        k_minus_1_tons_rdd = k_minus_1_tons_rdd.map(lambda x: x[0])\n",
    "        cartesian_rdd = singletons_rdd.cartesian(k_minus_1_tons_rdd)\n",
    "        # keep from cartesian the itemsets that are k long\n",
    "        cartesian_k_rdd = cartesian.map(lambda x: x[0].union(x[1])).filter(lambda x: len(x) == k)\n",
    "        cartesian_k = cartesian_k.collect()\n",
    "        # remove duplicate sets (could have due to cartesian)\n",
    "        cartesian_k_no_dupl = [set(item) for item in set(frozenset(item) for item in cartesian_k)]\n",
    "        # make candidate list with structure\n",
    "        candidates_list = [(x, 0) for idx, x in enumerate(cartesian_k_no_dupl)]\n",
    "        candidates = spark.sparkContext.parallelize(candidates_list)\n",
    "        # unpruned lenght\n",
    "        n_before_prune = candidates.count()\n",
    "        # save as ckpt\n",
    "        np.save(f'ckpt/candidates_notpruned_k_{k}', np.array(candidates.collect()))\n",
    "\n",
    "        # prune candidates: only keep the ones whose all subsets are also frequent itemsets \n",
    "        # subsets are of legnth=1,...,k-1\n",
    "        for i in range(1, k):\n",
    "            # k choose i, n_comb is the number of possible subsets in  \n",
    "            n_comb = comb(k,i)\n",
    "            # get frequent itemsets of lenght i\n",
    "            itemsets_i = itemsets_frequent_rdd.filter(lambda x: len(x[0]) == i).collect()\n",
    "            # for each candidate \n",
    "            candidates = \\\n",
    "                candidates.map(lambda x: (x[0], sum([len(x[0].intersection(s[0])) == i for s in itemsets_i])))\\\n",
    "                .filter(lambda x: n_comb == x[1])\n",
    "\n",
    "        candidates = candidates.map(lambda x: x[0]).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "        n_after_prune = candidates.count()\n",
    "\n",
    "        np.save(f'ckpt/candidates_k_{k}', np.array(candidates.collect()))\n",
    "\n",
    "        print(f\"proposing n={candidates.count()} candidates (n_pruned={n_before_prune - n_after_prune})\")\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def f(candidate, data_rdd_c, k):\n",
    "        return len(list(filter(lambda x: len(x) == k, map(lambda x: x & candidate, data_rdd_c))))\n",
    "\n",
    "def filter_itemsets(candidates_rdd, k, s, itemsets_frequent_rdd, from_ckpt=False):\n",
    "    if from_ckpt:\n",
    "        start_time = time.time()\n",
    "        print(\"Filtering loading from file...\")\n",
    "        res = np.load(f'ckpt/filtered_candidates_k_{k}_s_{s}.npy', allow_pickle=True)\n",
    "        res = spark.sparkContext.parallelize(res.tolist())\n",
    "        end_time = time.time()\n",
    "        print(f\"k={k}, t={end_time - start_time}, n={res.count()}\")\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        print(\"Staring filtering...\")\n",
    "\n",
    "        candidates = candidates_rdd.collect()\n",
    "\n",
    "        pool = mp.Pool(processes=nprocs)\n",
    "        supports = pool.starmap(f, [(c,data_rdd_c,k) for (idx, c) in candidates])\n",
    "\n",
    "        res = \\\n",
    "            spark.sparkContext.parallelize(candidates)\\\n",
    "            .filter(lambda x: s <= supports[x[0]]).map(lambda x: (x[1], supports[x[0]]))\n",
    "\n",
    "        np.save(f'ckpt/filtered_candidates_k_{k}_s_{s}', np.array(res.collect()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"k={k}, t={end_time - start_time}, n={res.count()}\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1000\n",
    "from_ckpt = True\n",
    "\n",
    "itemsets_frequent_rdd_1 = get_singletons(items_counts_rdd=items_counts_rdd, s=s)\n",
    "itemsets_frequent_rdd_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({448}, 1370),\n",
       " ({834}, 1373),\n",
       " ({240}, 1399),\n",
       " ({368}, 7828),\n",
       " ({274}, 2628),\n",
       " ({52}, 1983),\n",
       " ({630}, 1523),\n",
       " ({538}, 3982),\n",
       " ({704}, 1794),\n",
       " ({814}, 1672)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded proposed n=70125 candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, {413, 494}],\n",
       " [1, {874, 978}],\n",
       " [2, {701, 946}],\n",
       " [3, {335, 804}],\n",
       " [4, {576, 583}],\n",
       " [5, {242, 684}],\n",
       " [6, {597, 641}],\n",
       " [7, {581, 766}],\n",
       " [8, {335, 538}],\n",
       " [9, {39, 884}],\n",
       " [10, {516, 854}],\n",
       " [11, {115, 735}],\n",
       " [12, {126, 952}],\n",
       " [13, {854, 895}],\n",
       " [14, {682, 740}],\n",
       " [15, {774, 984}],\n",
       " [16, {468, 984}],\n",
       " [17, {738, 749}],\n",
       " [18, {675, 790}],\n",
       " [19, {529, 600}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_2 = construct_itemsets(k=2, itemsets_frequent_rdd=itemsets_frequent_rdd_1, from_ckpt=from_ckpt)\n",
    "candidates_2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering loading from file...\n",
      "k=2, t=0.004158735275268555, n=9\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "new_itemsets_frequent_rdd_2 = \\\n",
    "    filter_itemsets(candidates_rdd=candidates_2, k=2, s=s, itemsets_frequent_rdd=itemsets_frequent_rdd_1, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_2 = itemsets_frequent_rdd_1.union(new_itemsets_frequent_rdd_2)\n",
    "print(itemsets_frequent_rdd_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({448}, 1370),\n",
       " ({834}, 1373),\n",
       " ({240}, 1399),\n",
       " ({368}, 7828),\n",
       " ({274}, 2628),\n",
       " ({52}, 1983),\n",
       " ({630}, 1523),\n",
       " ({538}, 3982),\n",
       " ({704}, 1794),\n",
       " ({814}, 1672)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded proposed n=1 candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, {39, 704, 825}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_3 = construct_itemsets(k=3, itemsets_frequent_rdd=itemsets_frequent_rdd_2, from_ckpt=from_ckpt)\n",
    "candidates_3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering loading from file...\n",
      "k=3, t=0.0035021305084228516, n=1\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "new_itemsets_frequent_rdd_3 = \\\n",
    "    filter_itemsets(candidates_rdd=candidates_3, k=3, s=s, itemsets_frequent_rdd=itemsets_frequent_rdd_2, from_ckpt=from_ckpt)\n",
    "\n",
    "itemsets_frequent_rdd_3 = itemsets_frequent_rdd_2.union(new_itemsets_frequent_rdd_3)\n",
    "print(itemsets_frequent_rdd_3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposing n=0 candidates (n_pruned=372)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_4 = construct_itemsets(k=4, itemsets_frequent_rdd=itemsets_frequent_rdd_3, from_ckpt=False)\n",
    "candidates_4.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({448}, 1370),\n",
       " ({834}, 1373),\n",
       " ({240}, 1399),\n",
       " ({368}, 7828),\n",
       " ({274}, 2628),\n",
       " ({52}, 1983),\n",
       " ({630}, 1523),\n",
       " ({538}, 3982),\n",
       " ({704}, 1794),\n",
       " ({814}, 1672),\n",
       " ({120}, 4973),\n",
       " ({674}, 2527),\n",
       " ({854}, 2847),\n",
       " ({950}, 1463),\n",
       " ({964}, 1518),\n",
       " ({422}, 1255),\n",
       " ({738}, 2129),\n",
       " ({708}, 1090),\n",
       " ({294}, 1445),\n",
       " ({966}, 3921),\n",
       " ({978}, 1141),\n",
       " ({766}, 6265),\n",
       " ({104}, 1158),\n",
       " ({620}, 2100),\n",
       " ({798}, 3103),\n",
       " ({682}, 4132),\n",
       " ({970}, 2086),\n",
       " ({782}, 2767),\n",
       " ({658}, 1881),\n",
       " ({214}, 1893),\n",
       " ({350}, 3069),\n",
       " ({390}, 2685),\n",
       " ({530}, 1263),\n",
       " ({914}, 4037),\n",
       " ({280}, 2108),\n",
       " ({932}, 1786),\n",
       " ({192}, 2004),\n",
       " ({208}, 1483),\n",
       " ({720}, 3864),\n",
       " ({618}, 1337),\n",
       " ({496}, 1428),\n",
       " ({706}, 1923),\n",
       " ({878}, 2047),\n",
       " ({276}, 2479),\n",
       " ({960}, 2732),\n",
       " ({424}, 1448),\n",
       " ({490}, 1066),\n",
       " ({910}, 1695),\n",
       " ({130}, 1711),\n",
       " ({392}, 2420),\n",
       " ({862}, 3649),\n",
       " ({900}, 1165),\n",
       " ({78}, 2471),\n",
       " ({778}, 2514),\n",
       " ({572}, 1589),\n",
       " ({290}, 1793),\n",
       " ({614}, 3134),\n",
       " ({266}, 1022),\n",
       " ({458}, 1124),\n",
       " ({944}, 2794),\n",
       " ({888}, 3686),\n",
       " ({480}, 2309),\n",
       " ({70}, 2411),\n",
       " ({874}, 2237),\n",
       " ({204}, 2174),\n",
       " ({334}, 2146),\n",
       " ({504}, 1296),\n",
       " ({890}, 1437),\n",
       " ({810}, 1267),\n",
       " ({844}, 2814),\n",
       " ({846}, 1480),\n",
       " ({722}, 5845),\n",
       " ({310}, 1390),\n",
       " ({918}, 3012),\n",
       " ({326}, 1488),\n",
       " ({774}, 2046),\n",
       " ({526}, 2793),\n",
       " ({788}, 2386),\n",
       " ({198}, 1461),\n",
       " ({116}, 2193),\n",
       " ({946}, 1350),\n",
       " ({678}, 1329),\n",
       " ({780}, 2306),\n",
       " ({638}, 2288),\n",
       " ({242}, 2325),\n",
       " ({758}, 2860),\n",
       " ({956}, 3626),\n",
       " ({676}, 2717),\n",
       " ({790}, 1094),\n",
       " ({792}, 1306),\n",
       " ({522}, 2725),\n",
       " ({354}, 5835),\n",
       " ({548}, 2843),\n",
       " ({740}, 1632),\n",
       " ({296}, 2210),\n",
       " ({12}, 3415),\n",
       " ({684}, 5408),\n",
       " ({210}, 2009),\n",
       " ({884}, 1645),\n",
       " ({346}, 3470),\n",
       " ({234}, 1416),\n",
       " ({460}, 4438),\n",
       " ({746}, 1982),\n",
       " ({600}, 1192),\n",
       " ({28}, 1454),\n",
       " ({736}, 1470),\n",
       " ({744}, 2177),\n",
       " ({196}, 2096),\n",
       " ({494}, 5102),\n",
       " ({362}, 4388),\n",
       " ({628}, 1102),\n",
       " ({472}, 2125),\n",
       " ({58}, 1330),\n",
       " ({832}, 2062),\n",
       " ({580}, 1667),\n",
       " ({168}, 1538),\n",
       " ({632}, 1070),\n",
       " ({154}, 1447),\n",
       " ({988}, 1164),\n",
       " ({72}, 2852),\n",
       " ({10}, 1351),\n",
       " ({132}, 2641),\n",
       " ({32}, 4248),\n",
       " ({54}, 2595),\n",
       " ({348}, 1226),\n",
       " ({100}, 1749),\n",
       " ({500}, 1444),\n",
       " ({48}, 2472),\n",
       " ({126}, 1075),\n",
       " ({140}, 2687),\n",
       " ({112}, 2680),\n",
       " ({594}, 1516),\n",
       " ({606}, 2668),\n",
       " ({236}, 2618),\n",
       " ({952}, 1574),\n",
       " ({90}, 1875),\n",
       " ({122}, 1081),\n",
       " ({718}, 1238),\n",
       " ({516}, 1544),\n",
       " ({6}, 2149),\n",
       " ({110}, 1801),\n",
       " ({336}, 1071),\n",
       " ({574}, 1297),\n",
       " ({598}, 3219),\n",
       " ({470}, 4137),\n",
       " ({992}, 1116),\n",
       " ({162}, 1450),\n",
       " ({534}, 1531),\n",
       " ({378}, 1149),\n",
       " ({906}, 1444),\n",
       " ({912}, 1009),\n",
       " ({576}, 1337),\n",
       " ({716}, 1199),\n",
       " ({546}, 1050),\n",
       " ({8}, 3090),\n",
       " ({982}, 1640),\n",
       " ({984}, 1756),\n",
       " ({94}, 1201),\n",
       " ({692}, 4993),\n",
       " ({694}, 2847),\n",
       " ({800}, 1916),\n",
       " ({812}, 1518),\n",
       " ({414}, 1160),\n",
       " ({752}, 2578),\n",
       " ({998}, 2713),\n",
       " ({710}, 1044),\n",
       " ({170}, 1203),\n",
       " ({438}, 4511),\n",
       " ({332}, 1861),\n",
       " ({322}, 1154),\n",
       " ({928}, 1034),\n",
       " ({486}, 1547),\n",
       " ({440}, 1943),\n",
       " ({38}, 2402),\n",
       " ({784}, 1257),\n",
       " ({686}, 1495),\n",
       " ({540}, 1293),\n",
       " ({468}, 1089),\n",
       " ({886}, 3053),\n",
       " ({578}, 1290),\n",
       " ({510}, 3281),\n",
       " ({68}, 1601),\n",
       " ({860}, 1255),\n",
       " ({4}, 1394),\n",
       " ({804}, 1315),\n",
       " ({826}, 2022),\n",
       " ({394}, 1145),\n",
       " ({948}, 1149),\n",
       " ({308}, 1402),\n",
       " ({634}, 2492),\n",
       " ({688}, 1132),\n",
       " ({258}, 1036),\n",
       " ({450}, 2082),\n",
       " ({428}, 1021),\n",
       " ({550}, 1203),\n",
       " ({554}, 1114),\n",
       " ({366}, 1031),\n",
       " ({820}, 1473),\n",
       " ({775}, 3771),\n",
       " ({687}, 1762),\n",
       " ({561}, 2783),\n",
       " ({825}, 3085),\n",
       " ({25}, 1395),\n",
       " ({581}, 2943),\n",
       " ({39}, 4258),\n",
       " ({205}, 3605),\n",
       " ({401}, 3667),\n",
       " ({35}, 1984),\n",
       " ({733}, 1141),\n",
       " ({449}, 1890),\n",
       " ({937}, 4681),\n",
       " ({857}, 1588),\n",
       " ({895}, 3385),\n",
       " ({229}, 2281),\n",
       " ({883}, 4902),\n",
       " ({853}, 1804),\n",
       " ({283}, 4082),\n",
       " ({381}, 2959),\n",
       " ({143}, 1417),\n",
       " ({569}, 2835),\n",
       " ({809}, 2163),\n",
       " ({529}, 7057),\n",
       " ({947}, 3690),\n",
       " ({185}, 1529),\n",
       " ({227}, 1818),\n",
       " ({279}, 3014),\n",
       " ({675}, 2976),\n",
       " ({71}, 3507),\n",
       " ({597}, 2883),\n",
       " ({653}, 2634),\n",
       " ({183}, 3883),\n",
       " ({217}, 5375),\n",
       " ({795}, 3361),\n",
       " ({161}, 2320),\n",
       " ({175}, 2791),\n",
       " ({623}, 1845),\n",
       " ({177}, 4629),\n",
       " ({571}, 2902),\n",
       " ({125}, 1287),\n",
       " ({461}, 1498),\n",
       " ({921}, 2425),\n",
       " ({27}, 2165),\n",
       " ({579}, 2164),\n",
       " ({803}, 2237),\n",
       " ({147}, 1383),\n",
       " ({411}, 2047),\n",
       " ({523}, 2244),\n",
       " ({513}, 1287),\n",
       " ({43}, 1721),\n",
       " ({151}, 2611),\n",
       " ({419}, 5057),\n",
       " ({73}, 2179),\n",
       " ({469}, 1502),\n",
       " ({967}, 1695),\n",
       " ({975}, 1764),\n",
       " ({403}, 1722),\n",
       " ({789}, 4309),\n",
       " ({201}, 1029),\n",
       " ({805}, 1789),\n",
       " ({701}, 1283),\n",
       " ({171}, 1097),\n",
       " ({541}, 3735),\n",
       " ({487}, 3135),\n",
       " ({631}, 2793),\n",
       " ({935}, 1742),\n",
       " ({471}, 2894),\n",
       " ({735}, 1689),\n",
       " ({17}, 1683),\n",
       " ({763}, 1862),\n",
       " ({385}, 1676),\n",
       " ({145}, 4559),\n",
       " ({885}, 3043),\n",
       " ({617}, 2614),\n",
       " ({859}, 1242),\n",
       " ({841}, 1927),\n",
       " ({605}, 1652),\n",
       " ({829}, 6810),\n",
       " ({477}, 2462),\n",
       " ({649}, 1292),\n",
       " ({157}, 1140),\n",
       " ({5}, 1094),\n",
       " ({517}, 1201),\n",
       " ({115}, 1775),\n",
       " ({919}, 3710),\n",
       " ({641}, 1494),\n",
       " ({673}, 1635),\n",
       " ({489}, 3420),\n",
       " ({591}, 1241),\n",
       " ({651}, 1288),\n",
       " ({181}, 1235),\n",
       " ({573}, 1229),\n",
       " ({31}, 1666),\n",
       " ({871}, 2810),\n",
       " ({111}, 1171),\n",
       " ({981}, 1542),\n",
       " ({239}, 2742),\n",
       " ({21}, 2666),\n",
       " ({639}, 1572),\n",
       " ({765}, 1705),\n",
       " ({319}, 1371),\n",
       " ({521}, 1582),\n",
       " ({387}, 2089),\n",
       " ({285}, 2600),\n",
       " ({511}, 1015),\n",
       " ({583}, 1389),\n",
       " ({93}, 2777),\n",
       " ({941}, 1126),\n",
       " ({593}, 2601),\n",
       " ({1}, 1535),\n",
       " ({423}, 1412),\n",
       " ({69}, 2370),\n",
       " ({913}, 1939),\n",
       " ({797}, 2684),\n",
       " ({577}, 1695),\n",
       " ({611}, 1444),\n",
       " ({995}, 1521),\n",
       " ({509}, 3044),\n",
       " ({343}, 1599),\n",
       " ({527}, 1185),\n",
       " ({33}, 1460),\n",
       " ({989}, 1289),\n",
       " ({97}, 1466),\n",
       " ({793}, 3063),\n",
       " ({427}, 1856),\n",
       " ({37}, 1249),\n",
       " ({55}, 1959),\n",
       " ({897}, 1935),\n",
       " ({275}, 1692),\n",
       " ({259}, 1522),\n",
       " ({51}, 1612),\n",
       " ({45}, 1728),\n",
       " ({373}, 2007),\n",
       " ({665}, 1297),\n",
       " ({963}, 1327),\n",
       " ({349}, 2041),\n",
       " ({197}, 1230),\n",
       " ({749}, 1330),\n",
       " ({823}, 1031),\n",
       " ({413}, 2637),\n",
       " ({515}, 1166),\n",
       " ({567}, 1102),\n",
       " ({57}, 2743),\n",
       " ({41}, 1353),\n",
       " ({923}, 1753),\n",
       " ({377}, 1149),\n",
       " ({991}, 1268),\n",
       " ({899}, 1252),\n",
       " ({867}, 1530),\n",
       " ({563}, 1065),\n",
       " ({357}, 1142),\n",
       " ({361}, 1104),\n",
       " ({75}, 3151),\n",
       " ({265}, 1359),\n",
       " ({819}, 1257),\n",
       " ({663}, 2354),\n",
       " ({429}, 1037),\n",
       " ({843}, 1222),\n",
       " ({129}, 1547),\n",
       " ({887}, 1671),\n",
       " ({309}, 1262),\n",
       " ({325}, 1022),\n",
       " ({707}, 1354),\n",
       " ({105}, 1100),\n",
       " ({815}, 1358),\n",
       " ({661}, 2693),\n",
       " ({351}, 1641),\n",
       " ({405}, 1525),\n",
       " ({949}, 1414),\n",
       " ({163}, 1256),\n",
       " ({893}, 1947),\n",
       " ({335}, 1345),\n",
       " ({173}, 1080),\n",
       " ({85}, 1555),\n",
       " ({769}, 1622),\n",
       " ({207}, 1214),\n",
       " [{368, 829}, 1194],\n",
       " [{390, 722}, 1042],\n",
       " [{789, 829}, 1194],\n",
       " [{704, 825}, 1102],\n",
       " [{39, 704}, 1107],\n",
       " [{227, 390}, 1049],\n",
       " [{368, 682}, 1193],\n",
       " [{217, 346}, 1336],\n",
       " [{39, 825}, 1187],\n",
       " [{39, 704, 825}, 1035]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent_rdd_3.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{368, 829}, 1194],\n",
       " [{390, 722}, 1042],\n",
       " [{789, 829}, 1194],\n",
       " [{704, 825}, 1102],\n",
       " [{39, 704}, 1107],\n",
       " [{227, 390}, 1049],\n",
       " [{368, 682}, 1193],\n",
       " [{217, 346}, 1336],\n",
       " [{39, 825}, 1187],\n",
       " [{39, 704, 825}, 1035]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsets_frequent = itemsets_frequent_rdd_3.collect()\n",
    "itemsets_frequent_notsingle_rdd = itemsets_frequent_rdd_3.filter(lambda x: 1 < len(x[0]))\n",
    "itemsets_frequent_notsingle = itemsets_frequent_notsingle_rdd.collect()\n",
    "itemsets_frequent_notsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsets(myset):\n",
    "    max_ = len(myset)\n",
    "    min_ = 0\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    subsets = \\\n",
    "        [set(subset) for subset in list(chain.from_iterable(combinations(myset, r) for r in range(len(rule_set)+1)))]\n",
    "    valid_subsets = [subset for subset in subsets if 0 < len(subset) < max_]\n",
    "    return valid_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset_support = itemsets_frequent_notsingle[0]\n",
    "print(myset_support)\n",
    "myset = myset_support[0]\n",
    "subsets = get_subsets(myset)\n",
    "subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_sets = [(subset, myset.difference(subset)) for subset in subsets]\n",
    "rule_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_rule_set(rule_set, itemsets_frequent):\n",
    "    i = rule_set[0]\n",
    "    j = rule_set[1]\n",
    "    union = i.union(j)\n",
    "    \n",
    "    union_idx = \\\n",
    "        np.where(np.array(union) == np.array([itemset_frequent[0] for itemset_frequent in itemsets_frequent]))[0][0]\n",
    "    i_idx = \\\n",
    "        np.where(np.array(i) == np.array([itemset_frequent[0] for itemset_frequent in itemsets_frequent]))[0][0]\n",
    "    \n",
    "    union_support = itemsets_frequent[union_idx][1]\n",
    "    print(union_support)\n",
    "    i_support = itemsets_frequent[i_idx][1]\n",
    "    \n",
    "    conf = union_support / i_support\n",
    "    \n",
    "    return conf\n",
    "    \n",
    "rule_set = rule_sets[0]\n",
    "print(rule_set)\n",
    "get_conf_rule_set(rule_set, itemsets_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array({3,2}) == np.array([{2,1}, {3,4}, {2,3}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association_rules(itemsets_frequent, c_thresh=None):\n",
    "    itemsets_frequent_not_singleton = list(filter(lambda x: 1 < len(x[0]), itemsets_frequent))\n",
    "    print(itemsets_frequent_not_singleton)\n",
    "    \n",
    "    association_rules = []\n",
    "    \n",
    "    for itemset_frequent, union_support in itemsets_frequent_not_singleton:\n",
    "        print(itemset_frequent)\n",
    "        print(union_support)\n",
    "        \n",
    "        subsets = get_subsets(itemset_frequent)\n",
    "        \n",
    "        rule_sets = [(subset, itemset_frequent.difference(subset)) for subset in subsets]\n",
    "        \n",
    "        association_rule_sets = []\n",
    "        \n",
    "        for rule_set in rule_sets:\n",
    "            i = rule_set[0]\n",
    "            i_idx = \\\n",
    "                np.where(np.array(i) == np.array([itemset_frequent[0] for itemset_frequent in itemsets_frequent]))[0][0]\n",
    "            i_support = itemsets_frequent[i_idx][1]\n",
    "            \n",
    "            conf = union_support / i_support\n",
    "            \n",
    "            if c_thresh <= conf:\n",
    "                association_rule_sets.append((rule_set, conf))\n",
    "            \n",
    "        association_rules.append((itemset_frequent, association_rule_sets))\n",
    "        \n",
    "    return association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_thresh = 0.5\n",
    "association_rules = get_association_rules(itemsets_frequent=itemsets_frequent, c_thresh=c_thresh)\n",
    "print(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_association_rules(association_rules):\n",
    "    for frequent_itemset, association_rule_sets in association_rules:\n",
    "        print(f\"frequent itemset: {frequent_itemset}\")\n",
    "        \n",
    "        for association_rule_set, confidence in association_rule_sets:\n",
    "            print(f\"\\tassociation rule: {association_rule_set[0]} -> {association_rule_set[1]}\"\n",
    "                  f\" with confidence={confidence:.4f}\")\n",
    "        print()\n",
    "            \n",
    "pretty_print_association_rules(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
